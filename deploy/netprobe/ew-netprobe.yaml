# East-West Network Probe for Cross-Provider Comparison
# Measures in-cluster network latency and throughput between nodes
#
# Usage: kubectl apply -f ew-netprobe.yaml -n <namespace>
# Then run the client job to get results
---
apiVersion: v1
kind: Namespace
metadata:
  name: netprobe
  labels:
    app: netprobe
---
# iperf3 Server - runs continuously, waiting for client connections
apiVersion: apps/v1
kind: Deployment
metadata:
  name: iperf3-server
  namespace: netprobe
  labels:
    app: iperf3-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: iperf3-server
  template:
    metadata:
      labels:
        app: iperf3-server
    spec:
      containers:
        - name: iperf3
          # Use same image as client for compatibility
          image: nicolaka/netshoot:latest
          # Use --forceflush to avoid buffering issues, -D for daemon mode
          # Wrap in loop to restart server between tests (fixes "bad file descriptor" on GKE)
          command:
            - /bin/sh
            - -c
            - |
              echo "Starting iperf3 server in one-off loop mode..."
              while true; do
                iperf3 -s --one-off --forceflush 2>&1
                echo "Test completed, restarting server..."
                sleep 1
              done
          ports:
            - containerPort: 5201
              name: iperf
          resources:
            requests:
              cpu: "100m"
              memory: "64Mi"
            limits:
              cpu: "1000m"
              memory: "256Mi"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/gpu.present
                    operator: DoesNotExist
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: iperf3-client
                topologyKey: kubernetes.io/hostname
---
# Service to expose iperf3 server
apiVersion: v1
kind: Service
metadata:
  name: iperf3-server
  namespace: netprobe
spec:
  selector:
    app: iperf3-server
  ports:
    - port: 5201
      targetPort: 5201
      name: iperf
---
# iperf3 Client Job - runs tests and outputs JSON
apiVersion: batch/v1
kind: Job
metadata:
  name: iperf3-client
  namespace: netprobe
  labels:
    app: iperf3-client
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: iperf3-client
    spec:
      restartPolicy: Never
      containers:
        - name: iperf3-client
          # netshoot has iperf3, netcat, curl, and other network tools
          image: nicolaka/netshoot:latest
          command:
            - /bin/sh
            - -c
            - |
              echo '{"test_type": "east-west", "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'",' > /tmp/results.json
              
              # Get node info
              echo '"client_node": "'${NODE_NAME:-unknown}'",' >> /tmp/results.json
              
              # Wait for server to be ready with active probe
              echo "Waiting for iperf3 server..." >&2
              for i in $(seq 1 30); do
                if nc -z -w 2 iperf3-server 5201 2>/dev/null; then
                  echo "Server is ready after $i attempts" >&2
                  break
                fi
                echo "Waiting for server... attempt $i" >&2
                sleep 2
              done
              # Extra wait to ensure server is fully initialized
              sleep 5
              
              # TCP throughput test (10 seconds) with retry and longer waits
              echo "Running TCP throughput test..." >&2
              TCP_RESULT=""
              for attempt in 1 2 3; do
                # Wait for server to be in listening state
                sleep 3
                TCP_RESULT=$(iperf3 -c iperf3-server -t 10 -J --connect-timeout 10000 2>&1)
                if echo "$TCP_RESULT" | grep -q '"bits_per_second"'; then
                  echo "TCP test succeeded on attempt $attempt" >&2
                  break
                fi
                echo "TCP test attempt $attempt failed, waiting 10s for server to reset..." >&2
                sleep 10
              done
              echo "TCP raw result (first 500 chars):" >&2
              echo "$TCP_RESULT" | head -c 500 >&2
              echo "" >&2
              
              # Extract from the "end" summary section using jq (available in netshoot)
              TCP_BPS_RAW=$(echo "$TCP_RESULT" | jq -r '.end.sum_sent.bits_per_second // 0' 2>/dev/null || echo "0")
              TCP_RETRANSMITS=$(echo "$TCP_RESULT" | jq -r '.end.sum_sent.retransmits // 0' 2>/dev/null || echo "0")
              echo "Parsed TCP: BPS=$TCP_BPS_RAW RETRANS=$TCP_RETRANSMITS" >&2
              TCP_BPS=$(printf "%.0f" "$TCP_BPS_RAW")
              TCP_MBPS=$(awk "BEGIN {printf \"%.2f\", $TCP_BPS_RAW/1000000}")
              TCP_GBPS=$(awk "BEGIN {printf \"%.4f\", $TCP_BPS_RAW/1000000000}")
              
              echo '"tcp_throughput": {' >> /tmp/results.json
              echo '  "bits_per_second": '$TCP_BPS',' >> /tmp/results.json
              echo '  "mbps": '$TCP_MBPS',' >> /tmp/results.json
              echo '  "gbps": '$TCP_GBPS',' >> /tmp/results.json
              echo '  "retransmits": '${TCP_RETRANSMITS:-0}',' >> /tmp/results.json
              echo '  "duration_seconds": 10' >> /tmp/results.json
              echo '},' >> /tmp/results.json
              
              # Wait for server to restart after TCP test
              echo "Waiting for server to reset..." >&2
              sleep 5
              
              # UDP jitter test (5 seconds, 100Mbps target)
              echo "Running UDP jitter test..." >&2
              UDP_RESULT=$(iperf3 -c iperf3-server -u -b 100M -t 5 -J --connect-timeout 10000 2>&1)
              echo "UDP raw (first 300 chars):" >&2
              echo "$UDP_RESULT" | head -c 300 >&2
              echo "" >&2
              
              # Extract UDP metrics - try different paths since iperf3 output varies
              # Server-side stats have jitter, check .end.sum or .end.streams[0].udp
              UDP_JITTER=$(echo "$UDP_RESULT" | jq -r '(.end.sum.jitter_ms // .end.streams[0].udp.jitter_ms // 0)' 2>/dev/null || echo "0")
              UDP_LOST=$(echo "$UDP_RESULT" | jq -r '(.end.sum.lost_packets // .end.streams[0].udp.lost_packets // 0)' 2>/dev/null || echo "0")
              UDP_TOTAL=$(echo "$UDP_RESULT" | jq -r '(.end.sum.packets // .end.streams[0].udp.packets // 1)' 2>/dev/null || echo "1")
              # Handle null/empty values
              UDP_JITTER=${UDP_JITTER:-0}
              UDP_LOST=${UDP_LOST:-0}
              UDP_TOTAL=${UDP_TOTAL:-1}
              [ "$UDP_JITTER" = "null" ] && UDP_JITTER=0
              [ "$UDP_LOST" = "null" ] && UDP_LOST=0
              [ "$UDP_TOTAL" = "null" ] && UDP_TOTAL=1
              UDP_LOSS_PCT=$(awk "BEGIN {printf \"%.4f\", $UDP_LOST*100/$UDP_TOTAL}")
              echo "Parsed UDP: jitter=$UDP_JITTER lost=$UDP_LOST total=$UDP_TOTAL loss=$UDP_LOSS_PCT%" >&2
              
              echo '"udp_jitter": {' >> /tmp/results.json
              echo '  "jitter_ms": '${UDP_JITTER:-0}',' >> /tmp/results.json
              echo '  "lost_packets": '$UDP_LOST',' >> /tmp/results.json
              echo '  "total_packets": '$UDP_TOTAL',' >> /tmp/results.json
              echo '  "loss_percent": '$UDP_LOSS_PCT',' >> /tmp/results.json
              echo '  "target_mbps": 100,' >> /tmp/results.json
              echo '  "duration_seconds": 5' >> /tmp/results.json
              echo '},' >> /tmp/results.json
              
              # Latency test using TCP connection timing (ICMP ping often blocked)
              echo "Running latency test with TCP connect..." >&2
              
              LATENCIES=""
              for i in 1 2 3 4 5 6 7 8 9 10; do
                # Use netcat to measure TCP connect time
                START_MS=$(date +%s%3N)
                nc -z -w 1 iperf3-server 5201 2>/dev/null
                END_MS=$(date +%s%3N)
                LAT_MS=$((END_MS - START_MS))
                if [ $LAT_MS -lt 1000 ] && [ $LAT_MS -gt 0 ]; then
                  LATENCIES="$LATENCIES $LAT_MS"
                fi
              done
              echo "Raw latencies (ms):$LATENCIES" >&2
              
              # Calculate min/avg/max
              if [ -n "$LATENCIES" ]; then
                STATS=$(echo "$LATENCIES" | awk '{
                  n=split($0, a, " ")
                  if(n==0) { print "0 0 0"; exit }
                  min=999999; max=0; sum=0; count=0
                  for(i=1; i<=n; i++) {
                    if(a[i]+0 > 0) {
                      if(a[i] < min) min=a[i]
                      if(a[i] > max) max=a[i]
                      sum+=a[i]; count++
                    }
                  }
                  if(count>0) printf "%.3f %.3f %.3f", min, sum/count, max
                  else print "0 0 0"
                }')
                MIN_LAT=$(echo $STATS | awk '{print $1}')
                AVG_LAT=$(echo $STATS | awk '{print $2}')
                MAX_LAT=$(echo $STATS | awk '{print $3}')
              else
                MIN_LAT=0
                AVG_LAT=0
                MAX_LAT=0
              fi
              echo "Parsed latency: min=$MIN_LAT avg=$AVG_LAT max=$MAX_LAT ms" >&2
              
              echo '"latency": {' >> /tmp/results.json
              echo '  "min_ms": '${MIN_LAT:-0}',' >> /tmp/results.json
              echo '  "avg_ms": '${AVG_LAT:-0}',' >> /tmp/results.json
              echo '  "max_ms": '${MAX_LAT:-0}',' >> /tmp/results.json
              echo '  "samples": 10' >> /tmp/results.json
              echo '}' >> /tmp/results.json
              
              echo '}' >> /tmp/results.json
              
              # Output final JSON
              cat /tmp/results.json
              
              # Push metrics to Pushgateway (if available in cluster)
              PUSHGATEWAY_URL="${PUSHGATEWAY_URL:-http://prometheus-pushgateway.monitoring.svc.cluster.local:9091}"
              PROVIDER="${PROVIDER:-unknown}"
              
              echo "Pushing metrics to Pushgateway: $PUSHGATEWAY_URL" >&2
              
              # Build Prometheus text format
              cat > /tmp/metrics.txt << METRICS_EOF
              # HELP ew_tcp_throughput_gbps East-West TCP throughput in Gbps
              # TYPE ew_tcp_throughput_gbps gauge
              ew_tcp_throughput_gbps{provider="$PROVIDER"} $TCP_GBPS
              
              # HELP ew_tcp_throughput_bps East-West TCP throughput in bps
              # TYPE ew_tcp_throughput_bps gauge
              ew_tcp_throughput_bps{provider="$PROVIDER"} ${TCP_BPS:-0}
              
              # HELP ew_tcp_retransmits East-West TCP retransmits
              # TYPE ew_tcp_retransmits gauge
              ew_tcp_retransmits{provider="$PROVIDER"} ${TCP_RETRANSMITS:-0}
              
              # HELP ew_udp_jitter_ms East-West UDP jitter in ms
              # TYPE ew_udp_jitter_ms gauge
              ew_udp_jitter_ms{provider="$PROVIDER"} ${UDP_JITTER:-0}
              
              # HELP ew_udp_loss_percent East-West UDP packet loss percent
              # TYPE ew_udp_loss_percent gauge
              ew_udp_loss_percent{provider="$PROVIDER"} ${UDP_LOSS_PCT:-0}
              
              # HELP ew_latency_avg_ms East-West average latency in ms
              # TYPE ew_latency_avg_ms gauge
              ew_latency_avg_ms{provider="$PROVIDER"} ${AVG_LAT:-0}
              
              # HELP ew_latency_min_ms East-West min latency in ms
              # TYPE ew_latency_min_ms gauge
              ew_latency_min_ms{provider="$PROVIDER"} ${MIN_LAT:-0}
              
              # HELP ew_latency_max_ms East-West max latency in ms
              # TYPE ew_latency_max_ms gauge
              ew_latency_max_ms{provider="$PROVIDER"} ${MAX_LAT:-0}
              METRICS_EOF
              
              # Remove leading whitespace from metrics file
              sed -i 's/^[[:space:]]*//' /tmp/metrics.txt
              
              # Push to Pushgateway (ignore errors if not available)
              if curl -s --connect-timeout 5 -X POST "$PUSHGATEWAY_URL/metrics/job/east_west_probe/provider/$PROVIDER" --data-binary @/tmp/metrics.txt; then
                echo "Metrics pushed successfully" >&2
              else
                echo "Pushgateway not available, skipping metric push" >&2
              fi
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: PROVIDER
              value: "unknown"  # Override via kubectl set env or kustomize
            - name: PUSHGATEWAY_URL
              value: "http://prometheus-pushgateway.monitoring.svc.cluster.local:9091"
          resources:
            requests:
              cpu: "100m"
              memory: "64Mi"
            limits:
              cpu: "500m"
              memory: "128Mi"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/gpu.present
                    operator: DoesNotExist
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: iperf3-server
              topologyKey: kubernetes.io/hostname
